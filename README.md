# ğŸŒ Economic Insights ETL Pipeline

A robust ETL pipeline built in Python to extract, transform, and load economic, social, environmental, and infrastructure indicators from the World Bank Open Data API. This project demonstrates modular ETL design, API integration, data normalization, and file-based data warehousing in Parquet format.

---

## ğŸš€ Features

- Modular ETL pipelines for economic, social, environmental, and infrastructure indicators
- Connects to the World Bank API using country and indicator codes
- Extracts time-series data for multiple domains
- Transforms and cleans JSON responses into structured tabular format
- Saves cleaned data as partitioned Parquet files in bronze, silver, and gold layers
- Logging for ETL processes

---

## ğŸ—ï¸ Project Structure

economic-insights-etl/
â”œâ”€â”€ airflow/                  # Airflow orchestration (optional)
â”‚   â”œâ”€â”€ dags/
â”‚   â””â”€â”€ docker-compose.yml
â”œâ”€â”€ artifacts/                # Model artifacts or outputs
â”œâ”€â”€ backend/                  # Backend API (Node.js)
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ bronze/               # Raw data (partitioned Parquet)
â”‚   â”œâ”€â”€ silver/               # Cleaned/transformed data
â”‚   â””â”€â”€ gold/                 # Data for analytics/logic
â”œâ”€â”€ data_pipeline/
â”‚   â”œâ”€â”€ extract/              # Extraction scripts by domain
â”‚   â”‚   â”œâ”€â”€ bronze/
â”‚   â”‚   â”‚   â”œâ”€â”€ economic/
â”‚   â”‚   â”‚   â”œâ”€â”€ social/
â”‚   â”‚   â”‚   â”œâ”€â”€ environmental/
â”‚   â”‚   â”‚   â”œâ”€â”€ infrastructure/
â”‚   â”‚   â”‚   â””â”€â”€ util/
â”‚   â”‚   â”œâ”€â”€ silver/
â”‚   â”‚   â””â”€â”€ gold/
â”‚   â”œâ”€â”€ load/                 # Loaders (e.g., save_to_parquet.py)
â”‚   â”œâ”€â”€ transform/            # Transformers (e.g., clean_data.py)
â”‚   â””â”€â”€ utils/                # Utilities (e.g., world_bank_api_utils.py)
â”œâ”€â”€ frontend/                 # Frontend (React)
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ README.md
â””â”€â”€ main.py 

---

## ğŸ§ª How to Run

1. Create a virtual environment and install dependencies:
```bash
python -m venv venv
source venv/bin/activate  # or venv\Scripts\activate on Windows
pip install -r requirements.txt
```
2. Run the pipeline:
```bash
python -m data_pipeline.main_pipeline
```
3. Outputs will appear in:
```bash
data/bronze/ â€“ raw data

data/silver/ â€“ transformed data

data/gold/ â€“ data used in analytics/logic
```

---

## ğŸ“ Data Layers
- **Bronze:** Raw data as received from the API
- **Silver:** Cleaned and normalized data
- **Gold:** Data ready for analytics and business logic

---

## ğŸ“ Notes
- Airflow DAGs for orchestration are available in the `airflow/dags/` directory (optional)
- Backend and frontend folders are for API and UI extensions (optional)

---

ğŸ“š License
MIT License
